{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datascience import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'seqFISH_DASH/dapi_alignment_files/batch_dapi_TEMPLATE.py'\n",
    "orig = ['$FILENAME$']\n",
    "new = ['\"/central/groups/CaiLab/personal/Michal/raw/2021-12-06_p4p5p7_Neuro4181_5_repeat_pool2/\"']\n",
    "savenam = 'batch_dapi.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WRITE_FILE(filename,orig,new,savenam):\n",
    "    long_dir =filename.split('/')\n",
    "    cwd = '/'.join(long_dir[:len(long_dir)-1]) + '/'\n",
    "    \n",
    "    file1 = open(filename, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    file1.close()\n",
    "    \n",
    "    \n",
    "    new_lines = []\n",
    "    for line in Lines:\n",
    "        new_line = line\n",
    "        for orig_i, new_i in zip(orig,new):\n",
    "            if orig_i in line:\n",
    "                new_line = new_line.replace(orig_i, new_i)\n",
    "        new_lines.append(new_line)\n",
    "\n",
    "    if savenam in os.listdir(cwd):\n",
    "        os.system('rm -r '+cwd+savenam)\n",
    "        \n",
    "    file1 = open(cwd+savenam, 'w')\n",
    "    file1.writelines(new_lines)\n",
    "    file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'seqFISH_DASH/dapi_alignment_files/batch_dapi_TEMPLATE.py'\n",
    "orig = ['$FILENAME$']\n",
    "new = ['\"/central/groups/CaiLab/personal/Michal/raw/2021-12-06_p4p5p7_Neuro4181_5_repeat_pool2/\"']\n",
    "savenam = 'batch_dapi.py'\n",
    "\n",
    "WRITE_FILE(filename,orig,new,savenam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"/central/groups/CaiLab/personal/Michal/raw/2021-12-06_p4p5p7_Neuro4181_5_repeat_pool2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybes = np.asarray(os.listdir(wd))\n",
    "hyb = hybes[['Hyb' in i for i in hybes]][0]\n",
    "pos_nums = [int(i.replace('MMStack_Pos','').replace('.ome.tif','')) for i in os.listdir(wd +hyb)]\n",
    "hyb_nums = [int(i.replace('HybCycle_','')) for i in hybes[['Hyb' in i for i in hybes]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48,\n",
       " 77,\n",
       " 45,\n",
       " 69,\n",
       " 74,\n",
       " 50,\n",
       " 57,\n",
       " 65,\n",
       " 61,\n",
       " 71,\n",
       " 52,\n",
       " 63,\n",
       " 49,\n",
       " 60,\n",
       " 70,\n",
       " 76,\n",
       " 80,\n",
       " 59,\n",
       " 68,\n",
       " 75,\n",
       " 62,\n",
       " 54,\n",
       " 72,\n",
       " 55,\n",
       " 64,\n",
       " 73,\n",
       " 51,\n",
       " 79,\n",
       " 53,\n",
       " 78,\n",
       " 66,\n",
       " 47,\n",
       " 46,\n",
       " 58,\n",
       " 56,\n",
       " 67]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobIDs = [i.split(' ')[10] for i in os.popen('squeue -u hsekhon').readlines()[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('  21396400_[0-121]       ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \\n',\n",
       " '  21396400_[0-121]       any dapi_ali  hsekhon PD       0:00      1 (Priority) \\n',\n",
       " '          21395227       any sys/dash  hsekhon  R      35:50      1 hpc-83-14 \\n',\n",
       " '          21395000       any sys/dash  hsekhon  R      39:11      1 hpc-23-16 \\n']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.popen('squeue -u hsekhon').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21396400_[0-121]', '21395227', '21395000']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_dapi.py\n",
      "batch_dapi_TEMPLATE.py\n",
      "dapi_align.batch\n",
      "dapi_align_masks.py\n",
      "dapi_alignment_parallel.py\n",
      "dapi_align_TEMPLATE.batch\n",
      "util.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('cd seqFISH_DASH/dapi_alignment_files && ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batch_dapi.py\\n',\n",
       " 'batch_dapi_TEMPLATE.py\\n',\n",
       " 'dapi_align.batch\\n',\n",
       " 'dapi_align_masks.py\\n',\n",
       " 'dapi_alignment_parallel.py\\n',\n",
       " 'dapi_align_TEMPLATE.batch\\n',\n",
       " 'util.py\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.popen('cd seqFISH_DASH/dapi_alignment_files && ls').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'seqFISH_DASH/dapi_alignment_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slurm-21476578_24.out:slurmstepd: error: *** JOB 21476740 ON hpc-83-12 CANCELLED AT 2022-02-24T22:11:38 DUE TO TIME LIMIT ***\\nslurm-21476578_98.out:slurmstepd: error: *** JOB 21476946 ON hpc-80-04 CANCELLED AT 2022-02-24T22:16:43 DUE TO TIME LIMIT ***\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>gene_name</th> <th>Round1</th> <th>Round2</th> <th>Round3</th> <th>Round4</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>abt1     </td> <td>3     </td> <td>4     </td> <td>3     </td> <td>2     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>abtb1    </td> <td>2     </td> <td>2     </td> <td>3     </td> <td>7     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>acap3    </td> <td>1     </td> <td>4     </td> <td>6     </td> <td>3     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>aebp1    </td> <td>5     </td> <td>3     </td> <td>7     </td> <td>7     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>agap3    </td> <td>2     </td> <td>6     </td> <td>3     </td> <td>3     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>alx1     </td> <td>6     </td> <td>8     </td> <td>1     </td> <td>7     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>alx4     </td> <td>6     </td> <td>1     </td> <td>7     </td> <td>6     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ank1     </td> <td>1     </td> <td>3     </td> <td>1     </td> <td>5     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ankef1   </td> <td>6     </td> <td>3     </td> <td>7     </td> <td>8     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ankle2   </td> <td>8     </td> <td>7     </td> <td>1     </td> <td>8     </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (414 rows omitted)</p>"
      ],
      "text/plain": [
       "gene_name | Round1 | Round2 | Round3 | Round4\n",
       "abt1      | 3      | 4      | 3      | 2\n",
       "abtb1     | 2      | 2      | 3      | 7\n",
       "acap3     | 1      | 4      | 6      | 3\n",
       "aebp1     | 5      | 3      | 7      | 7\n",
       "agap3     | 2      | 6      | 3      | 3\n",
       "alx1      | 6      | 8      | 1      | 7\n",
       "alx4      | 6      | 1      | 7      | 6\n",
       "ank1      | 1      | 3      | 1      | 5\n",
       "ankef1    | 6      | 3      | 7      | 8\n",
       "ankle2    | 8      | 7      | 1      | 8\n",
       "... (414 rows omitted)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_nam='/home/hsekhon/shaan/Arun_Run_New/data_TF/codebooks/channel_640.csv'\n",
    "source= Table.from_df(pd.read_csv(src_nam).rename(columns={'Gene': 'gene_name', 'hyb1': 'Round1', 'hyb2': 'Round2', 'hyb3': 'Round3', 'hyb4': 'Round4'}))\n",
    "\n",
    "max_pseudo = int(max(np.asarray([source[1],source[2],source[3]]).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [np.arange(1,9),np.arange(1,9),np.arange(1,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_color_combos = set([\"-\".join([str(i) for i in list(i)]) for i in (itertools.product(*a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdbk = set([\"-\".join([str(int(a)),str(int(b)),str(int(c))]) for a,b,c in zip(source[1],source[2],source[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_codewords = [[int(j) for j in i.split('-')] for i in list(all_color_combos- cdbk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fourth(triplet):\n",
    "    triplet = [int(i) for i in triplet]\n",
    "    mod = sum(triplet)%max_pseudo\n",
    "    if mod == 0:\n",
    "        return max_pseudo\n",
    "    else:\n",
    "        return mod\n",
    "    \n",
    "\n",
    "\n",
    "fake_codewords_list = list(np.asarray(fake_codewords).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_codewords_list.append([get_fourth(i) for i in fake_codewords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_codewords_list = np.asarray(fake_codewords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4, 1, 2, 2, 2, 7, 3, 7, 1, 5, 7, 2, 4, 2, 1, 3, 1, 7, 5, 8, 3,\n",
       "        5, 2, 6, 4, 1, 7, 2, 1, 4, 6, 1, 8, 1, 8, 2, 5, 6, 5, 1, 5, 2, 1,\n",
       "        3, 3, 8, 2, 3, 7, 5, 3, 7, 7, 3, 2, 6, 1, 1, 7, 8, 3, 3, 4, 4, 7,\n",
       "        6, 4, 3, 5, 6, 5, 2],\n",
       "       [7, 4, 2, 2, 2, 2, 1, 4, 3, 6, 3, 8, 6, 5, 2, 6, 7, 1, 1, 4, 5, 6,\n",
       "        3, 2, 5, 5, 6, 3, 6, 7, 8, 7, 4, 6, 1, 5, 4, 6, 5, 4, 7, 5, 3, 5,\n",
       "        3, 3, 8, 6, 3, 2, 2, 1, 1, 4, 7, 4, 2, 1, 3, 2, 2, 7, 3, 4, 4, 7,\n",
       "        1, 2, 5, 1, 6, 3, 3],\n",
       "       [2, 4, 2, 7, 3, 5, 6, 6, 3, 5, 3, 5, 6, 2, 2, 8, 2, 4, 2, 2, 2, 1,\n",
       "        6, 1, 2, 4, 2, 7, 3, 3, 6, 6, 8, 8, 7, 7, 7, 8, 5, 8, 7, 5, 5, 1,\n",
       "        8, 1, 8, 8, 4, 5, 1, 1, 1, 8, 7, 2, 2, 1, 6, 7, 6, 5, 3, 5, 7, 7,\n",
       "        8, 3, 5, 8, 6, 4, 3],\n",
       "       [6, 4, 5, 3, 7, 1, 6, 5, 5, 4, 3, 4, 6, 3, 6, 7, 4, 6, 2, 3, 7, 2,\n",
       "        6, 5, 5, 5, 1, 1, 3, 3, 2, 3, 5, 6, 1, 4, 5, 3, 8, 1, 7, 7, 2, 7,\n",
       "        6, 7, 8, 8, 2, 6, 8, 5, 1, 3, 1, 8, 2, 3, 2, 8, 8, 7, 1, 5, 7, 5,\n",
       "        7, 1, 5, 6, 2, 4, 8]])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_codewords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_codewords =  [[int(j) for j in i.split('-')] for i in list(cdbk)]\n",
    "real_codewords_list = list(np.asarray(real_codewords).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_codewords_list.append([get_fourth(i) for i in real_codewords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_codewords_list = np.asarray(real_codewords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "round1 = list(fake_codewords_list[0])\n",
    "round2 = list(fake_codewords_list[1])\n",
    "round3 = list(fake_codewords_list[2])\n",
    "round4 = list(fake_codewords_list[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_names = ['fake' + str(i) for i in np.arange(len(fake_codewords_list[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>gene_name</th> <th>Round1</th> <th>Round2</th> <th>Round3</th> <th>Round4</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2810021j22rik</td> <td>3     </td> <td>2     </td> <td>7     </td> <td>4     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4931423n10rik</td> <td>5     </td> <td>5     </td> <td>1     </td> <td>3     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9130019o22rik</td> <td>2     </td> <td>1     </td> <td>7     </td> <td>2     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>aatf         </td> <td>3     </td> <td>8     </td> <td>2     </td> <td>5     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>aebp2        </td> <td>6     </td> <td>3     </td> <td>2     </td> <td>3     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>aff3         </td> <td>5     </td> <td>5     </td> <td>7     </td> <td>1     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ahr          </td> <td>3     </td> <td>3     </td> <td>2     </td> <td>8     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>aip          </td> <td>5     </td> <td>2     </td> <td>6     </td> <td>5     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>aire         </td> <td>1     </td> <td>8     </td> <td>3     </td> <td>4     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ankar        </td> <td>5     </td> <td>4     </td> <td>4     </td> <td>5     </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (429 rows omitted)</p>"
      ],
      "text/plain": [
       "gene_name     | Round1 | Round2 | Round3 | Round4\n",
       "2810021j22rik | 3      | 2      | 7      | 4\n",
       "4931423n10rik | 5      | 5      | 1      | 3\n",
       "9130019o22rik | 2      | 1      | 7      | 2\n",
       "aatf          | 3      | 8      | 2      | 5\n",
       "aebp2         | 6      | 3      | 2      | 3\n",
       "aff3          | 5      | 5      | 7      | 1\n",
       "ahr           | 3      | 3      | 2      | 8\n",
       "aip           | 5      | 2      | 6      | 5\n",
       "aire          | 1      | 8      | 3      | 4\n",
       "ankar         | 5      | 4      | 4      | 5\n",
       "... (429 rows omitted)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_book = Table().with_columns('gene_name',fake_names, 'Round1',np.asarray(round1,dtype='float'), 'Round2',np.asarray(round2,dtype='float'), 'Round3',np.asarray(round3, dtype='float'), 'Round4',np.asarray(round4,dtype='float')).to_df().set_index('gene_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.read_csv(src_nam).rename(columns={'Gene': 'gene_name', 'hyb1': 'Round1', 'hyb2': 'Round2', 'hyb3': 'Round3', 'hyb4': 'Round4'}).set_index('gene_name')\n",
    "           ,fake_book]).to_csv(src_nam.replace('codebooks', 'codebooks_converted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pexpect, time\n",
    "import os\n",
    "import time\n",
    "\n",
    "child = pexpect.spawn('ssh hsekhon@login.hpc.caltech.edu')\n",
    "child.logfile = open(\"mylog\", \"wb\")\n",
    "child.expect(\"assword\", timeout=120)\n",
    "child.sendline('ILikethebeatles2022')\n",
    "child.expect('(1-1)', timeout=120)\n",
    "child.sendline('1')\n",
    "child.expect(\"~]\", timeout=10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#child.expect(\"~]\", timeout=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success. Logging you in...\\nLast login: Thu Mar 17 14:34:23 2022 from secure-164.caltech.edu\\\\r\\n----------------------------------------------------------------------------\\n            THIS MACHINE IS FOR AUTHORIZED USE ONLY\\n_____________________________________________________________________________\\n\\nThis computer system is owned by the California Institute of Technology, and \\nits use is subject to the terms of Caltech\\\\\\'s Acceptable Use Policy: \\nhttps://hr.caltech.edu/resources/institute-policies/acceptable-use-electronic-resources-policy\\n\\nUnauthorized access is strictly forbidden.\\n\\nThe Resnick High Performance Computing Center is supported by the Resnick \\nSustainability Institute.  Additional support for the establishment of the \\nCenter was received from the Gordon and Betty Moore Foundation \\n\\n-----------------------------------------------------------------------------\\nTo report problems: send electronic mail to \"help-hpc@caltech.edu\"\\n-----------------------------------------------------------------------------\\nFor more information on using the cluster visit https://hpc.sites.caltech.edu\\n-----------------------------------------------------------------------------\\n\\n\\nUsage since 2021-10-01:\\n\\n--------------------------------------------------------------------------------\\nCluster/User/Account Utilization 2021-10-01T00:00:00 - 2022-03-16T23:59:59 (14428800 secs)\\nUsage reported in TRES Hours\\n--------------------------------------------------------------------------------\\n  Cluster     Login     Proper Name         Account      TRES Name      Used \\n--------- --------- --------------- --------------- -------------- --------- \\n  central   hsekhon    Shaan Sekhon          cailab            cpu    127675 \\n  central   hsekhon    Shaan Sekhon          cailab       gres/gpu       186 \\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_outs = str(child.before).split('\\\\r\\\\n')\n",
    "('\\n').join([i for i in str_outs[1:len(str_outs)-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "child.sendline('''python -c \"import sys;import os; print(os.listdir('shaan'))\"''')\n",
    "child.expect(\"~]\", timeout=10)\n",
    "\n",
    "filez = str(child.before).split('\\\\r\\\\n')[1].split('\\\\')\n",
    "\n",
    "filez = [i.replace(\"'\", \"\") for i in np.asarray(filez)[[i!= \"', \" and i!= '[' and i!= ']' for i in filez]]]\n",
    "filez[:len(filez)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['._.DS_Store',\n",
       " 'seqFISH_DASH',\n",
       " 'Check_Files',\n",
       " 'activate_cai_lab.sh',\n",
       " 'DAO_StarFinder_Michal',\n",
       " 'Arun_Run_New',\n",
       " '.ipynb_checkpoints',\n",
       " 'NEW_PIPELINE_2021-06-21_Neuro4181_5_noGel_pool1',\n",
       " 'smFISH_Neuro4181_DAW',\n",
       " 'Gene-Expression-Atlas',\n",
       " 'seqFISH_Arun-3',\n",
       " 'run_analysis',\n",
       " 'Simone-Registartion',\n",
       " 'seqFISH_Arun-3-DECODED',\n",
       " 'SAFE_seqFISH_DASH',\n",
       " '.DS_Store',\n",
       " 'DAO_StarFinder',\n",
       " 'seqFISH_DASH_THRESH',\n",
       " 'seqFISH_DASH_Pool2',\n",
       " 'DAO_StarFinder_Pipeline',\n",
       " 'Arun_SG_Data',\n",
       " 'Jina_Analysis',\n",
       " 'Optimize_SeqFishADGC',\n",
       " 'SeqFish-Michal']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filez = str(child.before).split('\\\\r\\\\n')[1].split('\\\\')\n",
    "\n",
    "filez = [i.replace(\"'\", \"\") for i in np.asarray(filez)[[i!= \"', \" and i!= '[' and i!= ']' for i in filez]]]\n",
    "filez[:len(filez)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'$ ls\\\\r\\\\n\\\\x1b[0m\\\\x1b[01;34mbin\\\\x1b[0m            \\\\x1b[01;34mminiconda3\\\\x1b[0m                                       \\\\x1b[01;34mSeqFISH_ADCG\\\\x1b[0m\\\\r\\\\n\\\\x1b[01;34mdata-pipeline\\\\x1b[0m  \\\\x1b[01;34mNew_Analysis_Pipeline\\\\x1b[0m                            \\\\x1b[01;34mseqFISH_DASH\\\\x1b[0m\\\\r\\\\n\\\\x1b[01;34mDesktop\\\\x1b[0m        \\\\x1b[01;34mNEW_PIPELINE_2021-06-21_Neuro4181_5_noGel_pool1\\\\x1b[0m  \\\\x1b[01;36mSeqFish-Michal\\\\x1b[0m\\\\r\\\\n\\\\x1b[01;34mDownloads\\\\x1b[0m      \\\\x1b[01;34mOld-Pipeline\\\\x1b[0m                                     \\\\x1b[01;36mshaan\\\\x1b[0m\\\\r\\\\n\\\\x1b[01;34mjre1.8.0_311\\\\x1b[0m   \\\\x1b[01;34mondemand\\\\x1b[0m                                         \\\\x1b[01;34mweb-ui\\\\x1b[0m\\\\r\\\\n\\\\x1b]0;hsekhon@login1:~\\\\x07(base) [hsekhon@login1 '\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "sometext = \n",
    "ansi_escape.sub('', sometext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 7-bit C1 ANSI sequences\n",
    "ansi_escape = re.compile(r'''\n",
    "    \\x1B  # ESC\n",
    "    (?:   # 7-bit C1 Fe (except CSI)\n",
    "        [@-Z\\\\-_]\n",
    "    |     # or [ for CSI, followed by a control sequence\n",
    "        \\[\n",
    "        [0-?]*  # Parameter bytes\n",
    "        [ -/]*  # Intermediate bytes\n",
    "        [@-~]   # Final byte\n",
    "    )\n",
    "''', re.VERBOSE)\n",
    "result = ansi_escape.sub('', str(child.before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'$ ls\\\\r\\\\n\\\\x1b[0m\\\\x1b[01;34mbin\\\\x1b[0m            \\\\x1b[01;34mminiconda3\\\\x1b[0m                                       \\\\x1b[01;34mSeqFISH_ADCG\\\\x1b[0m\\\\r\\\\n\\\\x1b[01;34mdata-pipeline\\\\x1b[0m  \\\\x1b[01;34mNew_Analysis_Pipeline\\\\x1b[0m                            \\\\x1b[01;34mseqFISH_DASH\\\\x1b[0m\\\\r\\\\n\\\\x1b[01;34mDesktop\\\\x1b[0m        \\\\x1b[01;34mNEW_PIPELINE_2021-06-21_Neuro4181_5_noGel_pool1\\\\x1b[0m  \\\\x1b[01;36mSeqFish-Michal\\\\x1b[0m\\\\r\\\\n\\\\x1b[01;34mDownloads\\\\x1b[0m      \\\\x1b[01;34mOld-Pipeline\\\\x1b[0m                                     \\\\x1b[01;36mshaan\\\\x1b[0m\\\\r\\\\n\\\\x1b[01;34mjre1.8.0_311\\\\x1b[0m   \\\\x1b[01;34mondemand\\\\x1b[0m                                         \\\\x1b[01;34mweb-ui\\\\x1b[0m\\\\r\\\\n\\\\x1b]0;hsekhon@login1:~\\\\x07(base) [hsekhon@login1 '\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WRITE_FILE(filename,orig,new,savenam):\n",
    "    long_dir =filename.split('/')\n",
    "    cwd = '/'.join(long_dir[:len(long_dir)-1]) + '/'\n",
    "    \n",
    "    file1 = open(filename, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    file1.close()\n",
    "    \n",
    "    \n",
    "    new_lines = []\n",
    "    for line in Lines:\n",
    "        new_line = line\n",
    "        for orig_i, new_i in zip(orig,new):\n",
    "            if orig_i in line:\n",
    "                new_line = new_line.replace(orig_i, new_i)\n",
    "        new_lines.append(new_line)\n",
    "    \n",
    "    child.sendline('rm -r '+cwd+savenam)\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "    \n",
    "    for new_line in new_lines:\n",
    "        child.sendline('echo -n \"$line\" >> $file'.replace('$file',cwd+savenam).replace('$line',new_line))\n",
    "        child.expect(\"~]\", timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WRITE_FILE(filename,orig,new,savenam):\n",
    "    long_dir =filename.split('/')\n",
    "    cwd = '/'.join(long_dir[:len(long_dir)-1]) + '/'\n",
    "    \n",
    "    file1 = open(filename, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    file1.close()\n",
    "    \n",
    "    \n",
    "    new_lines = []\n",
    "    for line in Lines:\n",
    "        new_line = line\n",
    "        for orig_i, new_i in zip(orig,new):\n",
    "            if orig_i in line:\n",
    "                new_line = new_line.replace(orig_i, new_i)\n",
    "        new_lines.append(new_line)\n",
    "    \n",
    "    child.sendline('rm -r '+cwd+savenam)\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "    \n",
    "    for new_line in new_lines:\n",
    "        child.sendline('echo -n \"$line\" >> $file'.replace('$file',cwd+savenam).replace('$line',new_line))\n",
    "        child.expect(\"~]\", timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'seqFISH_DASH/dapi_alignment_files/batch_dapi_TEMPLATE.py'\n",
    "orig = ['$FILENAME$']\n",
    "new = [('\"i\"').replace('i',\"TEST\")]\n",
    "savenam = 'batch_dapi.py'\n",
    "WRITE_FILE(filename,orig,new,savenam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'seqFISH_DASH/dapi_alignment_files/dapi_align_TEMPLATE.batch'\n",
    "orig = ['$POSRNG$']\n",
    "new = [str(1) +'-'+str(121)]\n",
    "savenam = 'dapi_align.batch' \n",
    "WRITE_FILE(filename,orig,new,savenam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUN_FILE(loc, filename):\n",
    "    child.sendline(\"cd \"+loc)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "    \n",
    "    child.sendline('''python -c \"import sys;import os; print(os.popen('$command$').read())\"'''.replace('$command$', \"sbatch \"+filename))\n",
    "    child.expect(\"Submitted batch\", timeout=10)\n",
    "    \n",
    "    str_outs = str(child.before)\n",
    "    \n",
    "    child.sendline(\"cd \")\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "    \n",
    "    return str_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUN_FILE(loc, filename):\n",
    "    child.sendline(\"cd \"+loc)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "    \n",
    "    child.sendline(\"sbatch \"+filename)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "        \n",
    "    child.sendline(\"cd \")\n",
    "    child.expect(\"~]\", timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'0;hsekhon@login1:~/seqFISH_DASH/dapi_alignment_files\\\\x07(base) [hsekhon@login1 dapi_alignment_files'\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = RUN_FILE('seqFISH_DASH/dapi_alignment_files/', 'dapi_align.batch')\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'$ scancel -u hsekhon\\\\r\\\\n\\\\x1b]0;hsekhon@login1:~\\\\x07(base) [hsekhon@login1 '\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child.sendline(\"scancel -u hsekhon\")\n",
    "child.expect(\"~]\", timeout=10)\n",
    "\n",
    "str(child.before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queue():\n",
    "    child.sendline('squeue -u hsekhon')\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "\n",
    "    str_outs = str(child.before).split('\\\\r\\\\n')\n",
    "    return ('\\n').join([i for i in str_outs[1:len(str_outs)-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) ',\n",
       " '          22143773       any     bash  hsekhon CG       0:04      1 hpc-90-13 ',\n",
       " '          22143774       any     bash  hsekhon  R       0:04      1 hpc-90-18 ',\n",
       " '          22143775       any     bash  hsekhon  R       0:04      1 hpc-90-18 ']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_queue().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid = 22143776\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22144192]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "logfile = open('mylog', 'r')\n",
    "loglines = logfile.readlines()\n",
    "logfile.close()\n",
    "\n",
    "jobids = [int(i.replace('\\n', '').replace('Submitted batch job ', '')) for i in np.asarray(loglines)[['Submitted batch job ' in i for i in loglines]]]\n",
    "jobids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobremover(jobid):\n",
    "    job_ID = str(int(jobid))\n",
    "    job_substrings =[i[:25] for i in get_queue().split('\\n')[1:]]\n",
    "    job_substrings = [i.replace(' ', '') for i in job_substrings]\n",
    "    for job_substring in job_substrings:\n",
    "        #print(job_ID,job_substrings)\n",
    "        if job_ID in job_substring:\n",
    "            child.sendline('scancel ' + job_substring)\n",
    "            child.expect(\"~]\", timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_substrings =[i[:25] for i in os.popen('squeue -u '+getpass.getuser()).readlines()[1:]]\n",
    "job_substrings = [i.replace(' ', '') for i in job_substrings]\n",
    "\n",
    "for job_substring in job_substrings:\n",
    "    #print(job_ID,job_substrings)\n",
    "    if job_ID in job_substring:\n",
    "        os.system('scancel ' + job_substring)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grep(path):\n",
    "    child.sendline('cd '+path)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "\n",
    "    child.sendline(\"grep -i error slurm*\")\n",
    "    child.expect(\"]\", timeout=10)\n",
    "\n",
    "    child.sendline('cd ')\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "    out_val = str(child.before).replace('grep -i error slurm*','')\n",
    "    return out_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'$ grep -i error slurm*\\\\r\\\\ngrep: slurm*: No such file or directory\\\\r\\\\n\\\\x1b]0;hsekhon@login1:~/seqFISH_DASH/dapi_alignment_files\\\\x07(base) [hsekhon@login1 dapi_alignment_files]$ cd \\\\r\\\\n\\\\x1b]0;hsekhon@login1:~\\\\x07(base) [hsekhon@login1 '\""
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(out_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'$ grep -i error slurm*\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_16.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148379 ON hpc-90-09 CANCELLED AT 2022-03-22T18:39:42 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_17.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148380 ON hpc-24-11 CANCELLED AT 2022-03-22T18:39:42 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_18.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148381 ON hpc-24-11 CANCELLED AT 2022-03-22T18:39:44 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_19.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148382 ON hpc-24-13 CANCELLED AT 2022-03-22T18:39:44 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_20.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148383 ON hpc-24-13 CANCELLED AT 2022-03-22T18:39:45 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_21.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148384 ON hpc-83-18 CANCELLED AT 2022-03-22T18:39:45 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_22.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148385 ON hpc-81-26 CANCELLED AT 2022-03-22T18:39:45 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_23.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148386 ON hpc-81-26 CANCELLED AT 2022-03-22T18:39:47 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_24.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148387 ON hpc-83-07 CANCELLED AT 2022-03-22T18:39:47 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_25.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148388 ON hpc-83-07 CANCELLED AT 2022-03-22T18:39:47 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_26.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148389 ON hpc-83-27 CANCELLED AT 2022-03-22T18:39:47 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_27.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148390 ON hpc-83-27 CANCELLED AT 2022-03-22T18:39:47 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_28.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148391 ON hpc-81-33 CANCELLED AT 2022-03-22T18:39:49 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_29.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148392 ON hpc-81-33 CANCELLED AT 2022-03-22T18:39:49 ***\\r\\n\\x1b[35m\\x1b[Kslurm-22148361_30.out\\x1b[m\\x1b[K\\x1b[36m\\x1b[K:\\x1b[m\\x1b[Kslurmstepd: \\x1b[01;31m\\x1b[Kerror\\x1b[m\\x1b[K: *** JOB 22148393 ON hpc-80-33 CANCELLED AT 2022-03-22T18:39:50 ***\\r\\n\\x1b]0;hsekhon@login1:~/seqFISH_DASH/preprocessing_files\\x07(base) [hsekhon@login1 preprocessing_files]$ cd \\r\\n\\x1b]0;hsekhon@login1:~\\x07(base) [hsekhon@login1 '"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child.before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lisdir(dir):\n",
    "    child.sendline('''python -c \"import sys;import os; print(os.listdir('$dir$'))\"'''.replace('$dir$', dir))\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "\n",
    "    filez = str(child.before).split('\\\\r\\\\n')[1].split('\\\\')\n",
    "\n",
    "    filez = [i.replace(\"'\", \"\") for i in np.asarray(filez)[[i!= \"', \" and i!= '[' and i!= ']' for i in filez]]]\n",
    "    return filez[:len(filez)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['root.file',\n",
       " '.ipynb_checkpoints',\n",
       " 'batch_dapi_TEMPLATE.py',\n",
       " 'util.py',\n",
       " 'dapi_alignment_parallel.py',\n",
       " 'dapi_align_TEMPLATE.batch',\n",
       " 'batch_dapi.py',\n",
       " 'test.py',\n",
       " 'dapi_align_masks.py',\n",
       " '__pycache__',\n",
       " 'dapi_align.batch']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lisdir('seqFISH_DASH/dapi_alignment_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shaansekhon/Desktop/SeqFISH_DASH_GUI'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lisdir(directory):\n",
    "    child.sendline('''python -c \"import sys;import os; print(os.listdir('$dir$'))\"'''.replace('$dir$', directory))\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "\n",
    "    filez = str(child.before).split('\\\\r\\\\n')[1].split('\\\\')\n",
    "\n",
    "    filez = [i.replace(\"'\", \"\") for i in np.asarray(filez)[[i!= \"', \" and i!= '[' and i!= ']' for i in filez]]]\n",
    "    return filez[:len(filez)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUCCESSFUL_notebook_pyfiles',\n",
       " 'HybCycle_48',\n",
       " 'HybCycle_77',\n",
       " 'HybCycle_45',\n",
       " 'HybCycle_69',\n",
       " 'HybCycle_74',\n",
       " 'HybCycle_50',\n",
       " 'HybCycle_57',\n",
       " 'HybCycle_65',\n",
       " 'HybCycle_61',\n",
       " 'HybCycle_71',\n",
       " 'HybCycle_52',\n",
       " 'HybCycle_63',\n",
       " 'HybCycle_49',\n",
       " 'HybCycle_60',\n",
       " 'HybCycle_70',\n",
       " 'HybCycle_76',\n",
       " 'HybCycle_80',\n",
       " 'Labeled_Images',\n",
       " 'notebook_pyfiles',\n",
       " 'HybCycle_59',\n",
       " 'HybCycle_68',\n",
       " 'HybCycle_75',\n",
       " 'HybCycle_62',\n",
       " 'HybCycle_54',\n",
       " 'HybCycle_72',\n",
       " 'HybCycle_55',\n",
       " 'HybCycle_64',\n",
       " 'HybCycle_73',\n",
       " 'HybCycle_51',\n",
       " 'HybCycle_79',\n",
       " 'HybCycle_53',\n",
       " 'HybCycle_78',\n",
       " 'HybCycle_66',\n",
       " 'initial_background',\n",
       " 'HybCycle_47',\n",
       " 'HybCycle_46',\n",
       " 'HybCycle_58',\n",
       " 'HybCycle_56',\n",
       " 'HybCycle_67']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lisdir('/central/groups/CaiLab/personal/Michal/raw/2021-12-06_p4p5p7_Neuro4181_5_repeat_pool2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lisdir(directory):\n",
    "    child.sendline('''python -c \"import sys;import os; print(os.listdir('$dir$'))\"'''.replace('$dir$', directory))\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "\n",
    "    filez = str(child.before).split('\\\\r\\\\n')[1].split('\\\\')\n",
    "\n",
    "    filez = [i.replace(\"'\", \"\") for i in np.asarray(filez)[[i!= \"', \" and i!= '[' and i!= ']' for i in filez]]]\n",
    "    return filez[:len(filez)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lisdir_new(path):\n",
    "    child.sendline('cd '+path)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "\n",
    "    child.sendline(\"ls\")\n",
    "    child.expect(\"]\", timeout=10)\n",
    "\n",
    "    child.sendline('cd ')\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "    out_val = str(child.before)\n",
    "    \n",
    "    sub_ls = out_val.split('\\\\r\\\\n')\n",
    "    all_sublists = []\n",
    "\n",
    "    for i in sub_ls:\n",
    "        j = i.split('\\\\') \n",
    "        [all_sublists.append(jj) for jj in j]\n",
    "    all_sublists_short = all_sublists[1:len(all_sublists)-6]\n",
    "    new_lis = [i[10:] for i in np.asarray(all_sublists_short)[['x1b[01;3' in i for i in all_sublists_short]]]\n",
    "\n",
    "\n",
    "\n",
    "    return new_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lisdir_new(path):\n",
    "    child.sendline('cd '+path)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "\n",
    "    child.sendline(\"ls\")\n",
    "    child.expect(\"]\", timeout=10)\n",
    "\n",
    "    child.sendline('cd ')\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "    out_val = str(child.before)\n",
    "    return out_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'$ ls\\\\r\\\\nbatch_dapi.py           \\\\x1b[0m\\\\x1b[01;32mdapi_align_masks.py\\\\x1b[0m         root.file\\\\r\\\\n\\\\x1b[01;32mbatch_dapi_TEMPLATE.py\\\\x1b[0m  \\\\x1b[01;32mdapi_alignment_parallel.py\\\\x1b[0m  test.py\\\\r\\\\ncleaner.py              \\\\x1b[01;32mdapi_align_TEMPLATE.batch\\\\x1b[0m   \\\\x1b[01;32mutil.py\\\\x1b[0m\\\\r\\\\ndapi_align.batch        \\\\x1b[01;34m__pycache__\\\\x1b[0m\\\\r\\\\n\\\\x1b]0;hsekhon@login1:~/seqFISH_DASH/dapi_alignment_files\\\\x07(base) [hsekhon@login1 dapi_alignment_files]$ cd \\\\r\\\\n\\\\x1b]0;hsekhon@login1:~\\\\x07(base) [hsekhon@login1 '\""
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lisdir_new('seqFISH_DASH/dapi_alignment_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ls = get_lisdir_new('seqFISH_DASH/dapi_alignment_files/').split('\\\\r\\\\n')\n",
    "all_sublists = []\n",
    "\n",
    "for i in sub_ls:\n",
    "    j = i.split('\\\\') \n",
    "    [all_sublists.append(jj) for jj in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sublists_short = all_sublists[1:len(all_sublists)-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dapi_align_masks.py',\n",
       " 'batch_dapi_TEMPLATE.py',\n",
       " 'dapi_alignment_parallel.py',\n",
       " 'dapi_align_TEMPLATE.batch',\n",
       " 'util.py',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lis = [i[10:] for i in np.asarray(all_sublists_short)[['x1b[01;3' in i for i in all_sublists_short]]]\n",
    "new_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dapi_align_masks.py',\n",
       " 'batch_dapi_TEMPLATE.py',\n",
       " 'dapi_alignment_parallel.py',\n",
       " 'dapi_align_TEMPLATE.batch',\n",
       " 'util.py',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'seqFISH_DASH/dapi_alignment_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['__pycache__', 'batch_dapi_TEMPLATE.py',\n",
       "        'dapi_align_TEMPLATE.batch', 'dapi_align_masks.py',\n",
       "        'dapi_alignment_parallel.py', 'util.py'], dtype='<U26'),\n",
       " array(['.ipynb_checkpoints', '__pycache__', 'batch_dapi.py',\n",
       "        'batch_dapi_TEMPLATE.py', 'cleaner.py', 'dapi_align.batch',\n",
       "        'dapi_align_TEMPLATE.batch', 'dapi_align_masks.py',\n",
       "        'dapi_alignment_parallel.py', 'root.file', 'test.py', 'util.py'],\n",
       "       dtype='<U26'))"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(get_lisdir_new(path)) , np.sort(get_lisdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pexpect, time\n",
    "import os\n",
    "import time\n",
    "\n",
    "child = pexpect.spawn('ssh hsekhon@login.hpc.caltech.edu')\n",
    "child.logfile = open(\"mylog\", \"wb\")\n",
    "child.expect(\"assword\", timeout=120)\n",
    "child.sendline('ILikethebeatles2022')\n",
    "child.expect('(1-1)', timeout=120)\n",
    "child.sendline('1')\n",
    "child.expect(\"~]\", timeout=10)\n",
    "\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "import os\n",
    "import numpy as np\n",
    "from tkinter.font import Font\n",
    "from tkinter import ttk\n",
    "#from helper import *\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "global jobID_O_dapialign,jobID_O_preprocessing\n",
    "jobID_O_dapialign = None\n",
    "jobID_O_preprocessing = None\n",
    "\n",
    "import pexpect, time\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def get_queue():\n",
    "    child.sendline('squeue -u hsekhon')\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "\n",
    "    str_outs = str(child.before).split('\\\\r\\\\n')\n",
    "    #str_outs = np.asarray(str_outs)[['squeue -u' not in i for i in str_outs]]\n",
    "    return ('\\n').join([i for i in str_outs[1:len(str_outs)-1]])\n",
    "    \n",
    "\n",
    "def get_lisdir(directory):\n",
    "    child.sendline('''python -c \"import sys;import os; print(os.listdir('$dir$'))\"'''.replace('$dir$', directory))\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "\n",
    "    filez = str(child.before).split('\\\\r\\\\n')[1].split('\\\\')\n",
    "\n",
    "    filez = [i.replace(\"'\", \"\") for i in np.asarray(filez)[[i!= \"', \" and i!= '[' and i!= ']' for i in filez]]]\n",
    "    return filez[:len(filez)-1]\n",
    "    \n",
    "def clean_slurms(path):\n",
    "    \n",
    "    child.sendline(\"cd \"+path)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "    \n",
    "    run_command('rm slurm*')\n",
    "    \n",
    "    child.sendline(\"cd \")\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "    \n",
    "def RUN_FILE(loc, filename):\n",
    "    child.sendline(\"cd \"+loc)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "    \n",
    "    child.sendline(\"sbatch \"+filename)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "        \n",
    "    child.sendline(\"cd \")\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "\n",
    "def run_command(command):\n",
    "    child.sendline(command)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "    \n",
    "def get_grep(path):\n",
    "    child.sendline('cd '+path)\n",
    "    child.expect(\"]\", timeout=10)\n",
    "\n",
    "    child.sendline(\"grep -i error slurm*\")\n",
    "    child.expect(\"]\", timeout=10)\n",
    "\n",
    "    child.sendline('cd ')\n",
    "    child.expect(\"~]\", timeout=10)\n",
    "    out_val = str(child.before).replace('grep -i error slurm*','')\n",
    "    return out_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/central/groups/CaiLab/personal/Michal/raw/2021-12-06_p4p5p7_Neuro4181_5_repeat_pool2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUCCESSFUL_notebook_pyfiles',\n",
       " 'HybCycle_48',\n",
       " 'HybCycle_77',\n",
       " 'HybCycle_45',\n",
       " 'HybCycle_69',\n",
       " 'HybCycle_74',\n",
       " 'HybCycle_50',\n",
       " 'HybCycle_57',\n",
       " 'HybCycle_65',\n",
       " 'HybCycle_61',\n",
       " 'HybCycle_71',\n",
       " 'HybCycle_52',\n",
       " 'HybCycle_63',\n",
       " 'HybCycle_49',\n",
       " 'HybCycle_60',\n",
       " 'HybCycle_70',\n",
       " 'HybCycle_76',\n",
       " 'HybCycle_80',\n",
       " 'Labeled_Images',\n",
       " 'notebook_pyfiles',\n",
       " 'HybCycle_59',\n",
       " 'HybCycle_68',\n",
       " 'HybCycle_75',\n",
       " 'HybCycle_62',\n",
       " 'HybCycle_54',\n",
       " 'HybCycle_72',\n",
       " 'HybCycle_55',\n",
       " 'HybCycle_64',\n",
       " 'HybCycle_73',\n",
       " 'HybCycle_51',\n",
       " 'HybCycle_79',\n",
       " 'HybCycle_53',\n",
       " 'HybCycle_78',\n",
       " 'HybCycle_66',\n",
       " 'initial_background',\n",
       " 'HybCycle_47',\n",
       " 'HybCycle_46',\n",
       " 'HybCycle_58',\n",
       " 'HybCycle_56',\n",
       " 'HybCycle_67']"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lisdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUCCESSFUL_notebook_pyfiles',\n",
       " 'HybCycle_48',\n",
       " 'HybCycle_77',\n",
       " 'HybCycle_45',\n",
       " 'HybCycle_69',\n",
       " 'HybCycle_74',\n",
       " 'HybCycle_50',\n",
       " 'HybCycle_57',\n",
       " 'HybCycle_65',\n",
       " 'HybCycle_61',\n",
       " 'HybCycle_71',\n",
       " 'HybCycle_52',\n",
       " 'HybCycle_63',\n",
       " 'HybCycle_49',\n",
       " 'HybCycle_60',\n",
       " 'HybCycle_70',\n",
       " 'HybCycle_76',\n",
       " 'HybCycle_80',\n",
       " 'Labeled_Images',\n",
       " 'notebook_pyfiles',\n",
       " 'HybCycle_59',\n",
       " 'HybCycle_68',\n",
       " 'HybCycle_75',\n",
       " 'HybCycle_62',\n",
       " 'HybCycle_54',\n",
       " 'HybCycle_72',\n",
       " 'HybCycle_55',\n",
       " 'HybCycle_64',\n",
       " 'HybCycle_73',\n",
       " 'HybCycle_51',\n",
       " 'HybCycle_79',\n",
       " 'HybCycle_53',\n",
       " 'HybCycle_78',\n",
       " 'HybCycle_66',\n",
       " 'initial_background',\n",
       " 'HybCycle_47',\n",
       " 'HybCycle_46',\n",
       " 'HybCycle_58',\n",
       " 'HybCycle_56',\n",
       " 'HybCycle_67']"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child.sendline('''python -c \"import sys;import os; print(os.listdir('$dir$'))\"'''.replace('$dir$', path))\n",
    "child.expect(\"~]\", timeout=10)\n",
    "\n",
    "filez = str(child.before).split('\\\\r\\\\n')[1].split('\\\\')\n",
    "\n",
    "filez = [i.replace(\"'\", \"\") for i in np.asarray(filez)[[i!= \"', \" and i!= '[' and i!= ']' for i in filez]]]\n",
    "filez[:len(filez)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\\'$ python -c \"import sys;import os; print(os.listdir(\\\\\\'/c \\\\rentral/groups/CaiLab/personal/Michal/raw/2021-12-06_p4p5p7_Neuro4181_5_repeat_po \\\\rol2/\\\\\\'))\"\\\\r\\\\n[\\\\\\'SUCCESSFUL_notebook_pyfiles\\\\\\', \\\\\\'HybCycle_48\\\\\\', \\\\\\'HybCycle_77\\\\\\', \\\\\\'HybCycle_45\\\\\\', \\\\\\'HybCycle_69\\\\\\', \\\\\\'HybCycle_74\\\\\\', \\\\\\'HybCycle_50\\\\\\', \\\\\\'HybCycle_57\\\\\\', \\\\\\'HybCycle_65\\\\\\', \\\\\\'HybCycle_61\\\\\\', \\\\\\'HybCycle_71\\\\\\', \\\\\\'HybCycle_52\\\\\\', \\\\\\'HybCycle_63\\\\\\', \\\\\\'HybCycle_49\\\\\\', \\\\\\'HybCycle_60\\\\\\', \\\\\\'HybCycle_70\\\\\\', \\\\\\'HybCycle_76\\\\\\', \\\\\\'HybCycle_80\\\\\\', \\\\\\'Labeled_Images\\\\\\', \\\\\\'notebook_pyfiles\\\\\\', \\\\\\'HybCycle_59\\\\\\', \\\\\\'HybCycle_68\\\\\\', \\\\\\'HybCycle_75\\\\\\', \\\\\\'HybCycle_62\\\\\\', \\\\\\'HybCycle_54\\\\\\', \\\\\\'HybCycle_72\\\\\\', \\\\\\'HybCycle_55\\\\\\', \\\\\\'HybCycle_64\\\\\\', \\\\\\'HybCycle_73\\\\\\', \\\\\\'HybCycle_51\\\\\\', \\\\\\'HybCycle_79\\\\\\', \\\\\\'HybCycle_53\\\\\\', \\\\\\'HybCycle_78\\\\\\', \\\\\\'HybCycle_66\\\\\\', \\\\\\'initial_background\\\\\\', \\\\\\'HybCycle_47\\\\\\', \\\\\\'HybCycle_46\\\\\\', \\\\\\'HybCycle_58\\\\\\', \\\\\\'HybCycle_56\\\\\\', \\\\\\'HybCycle_67\\\\\\']\\\\r\\\\n\\\\x1b]0;hsekhon@login1:~\\\\x07(base) [hsekhon@login1 \\''"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(child.before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'seqFISH_DASH/dapi_alignment_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['root.file',\n",
       " 'cleaner.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'batch_dapi_TEMPLATE.py',\n",
       " 'util.py',\n",
       " 'dapi_alignment_parallel.py',\n",
       " 'dapi_align_TEMPLATE.batch',\n",
       " 'batch_dapi.py',\n",
       " 'test.py',\n",
       " 'dapi_align_masks.py',\n",
       " '__pycache__',\n",
       " 'dapi_align.batch']"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_lisdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_slurms('seqFISH_DASH/dapi_alignment_files')\n",
    "clean_slurms('seqFISH_DASH/preprocessing_files')\n",
    "\n",
    "clean_slurms('seqFISH_DASH/optimization_files/dot_detection_files')\n",
    "clean_slurms('seqFISH_DASH/optimization_files/decoding_files')\n",
    "\n",
    "clean_slurms('seqFISH_DASH/dot_detection_files')\n",
    "clean_slurms('seqFISH_DASH/decoding_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "    \n",
    "clean_slurms('seqFISH_DASH/dapi_alignment_files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUCCESSFUL_notebook_pyfiles',\n",
       " 'HybCycle_48',\n",
       " 'HybCycle_77',\n",
       " 'HybCycle_45',\n",
       " 'HybCycle_69',\n",
       " 'HybCycle_74',\n",
       " 'HybCycle_50',\n",
       " 'HybCycle_57',\n",
       " 'HybCycle_65',\n",
       " 'HybCycle_61',\n",
       " 'HybCycle_71',\n",
       " 'HybCycle_52',\n",
       " 'HybCycle_63',\n",
       " 'HybCycle_49',\n",
       " 'HybCycle_60',\n",
       " 'HybCycle_70',\n",
       " 'HybCycle_76',\n",
       " 'HybCycle_80',\n",
       " 'Labeled_Images',\n",
       " 'notebook_pyfiles',\n",
       " 'HybCycle_59',\n",
       " 'HybCycle_68',\n",
       " 'HybCycle_75',\n",
       " 'HybCycle_62',\n",
       " 'HybCycle_54',\n",
       " 'HybCycle_72',\n",
       " 'HybCycle_55',\n",
       " 'HybCycle_64',\n",
       " 'HybCycle_73',\n",
       " 'HybCycle_51',\n",
       " 'HybCycle_79',\n",
       " 'HybCycle_53',\n",
       " 'HybCycle_78',\n",
       " 'HybCycle_66',\n",
       " 'initial_background',\n",
       " 'HybCycle_47',\n",
       " 'HybCycle_46',\n",
       " 'HybCycle_58',\n",
       " 'HybCycle_56',\n",
       " 'HybCycle_67']"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child.sendline('''python -c \"import sys;import os; print(os.listdir('$dir$'))\"'''.replace('$dir$', path))\n",
    "child.expect(\"~]\", timeout=10)\n",
    "\n",
    "filez = str(child.before).split('\\\\r\\\\n')[1].split('\\\\')\n",
    "\n",
    "filez = [i.replace(\"'\", \"\") for i in np.asarray(filez)[[i!= \"', \" and i!= '[' and i!= ']' for i in filez]]]\n",
    "filez[:len(filez)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\\'$ python -c \"import sys;import os; print(os.listdir(\\\\\\'/c \\\\rentral/groups/CaiLab/personal/Michal/raw/2021-12-06_p4p5p7_Neuro4181_5_repeat_po \\\\rol2/\\\\\\'))\"\\\\r\\\\n[\\\\\\'SUCCESSFUL_notebook_pyfiles\\\\\\', \\\\\\'HybCycle_48\\\\\\', \\\\\\'HybCycle_77\\\\\\', \\\\\\'HybCycle_45\\\\\\', \\\\\\'HybCycle_69\\\\\\', \\\\\\'HybCycle_74\\\\\\', \\\\\\'HybCycle_50\\\\\\', \\\\\\'HybCycle_57\\\\\\', \\\\\\'HybCycle_65\\\\\\', \\\\\\'HybCycle_61\\\\\\', \\\\\\'HybCycle_71\\\\\\', \\\\\\'HybCycle_52\\\\\\', \\\\\\'HybCycle_63\\\\\\', \\\\\\'HybCycle_49\\\\\\', \\\\\\'HybCycle_60\\\\\\', \\\\\\'HybCycle_70\\\\\\', \\\\\\'HybCycle_76\\\\\\', \\\\\\'HybCycle_80\\\\\\', \\\\\\'Labeled_Images\\\\\\', \\\\\\'notebook_pyfiles\\\\\\', \\\\\\'HybCycle_59\\\\\\', \\\\\\'HybCycle_68\\\\\\', \\\\\\'HybCycle_75\\\\\\', \\\\\\'HybCycle_62\\\\\\', \\\\\\'HybCycle_54\\\\\\', \\\\\\'HybCycle_72\\\\\\', \\\\\\'HybCycle_55\\\\\\', \\\\\\'HybCycle_64\\\\\\', \\\\\\'HybCycle_73\\\\\\', \\\\\\'HybCycle_51\\\\\\', \\\\\\'HybCycle_79\\\\\\', \\\\\\'HybCycle_53\\\\\\', \\\\\\'HybCycle_78\\\\\\', \\\\\\'HybCycle_66\\\\\\', \\\\\\'initial_background\\\\\\', \\\\\\'HybCycle_47\\\\\\', \\\\\\'HybCycle_46\\\\\\', \\\\\\'HybCycle_58\\\\\\', \\\\\\'HybCycle_56\\\\\\', \\\\\\'HybCycle_67\\\\\\']\\\\r\\\\n\\\\x1b]0;hsekhon@login1:~\\\\x07(base) [hsekhon@login1 \\''"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(child.before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 3D array with LSTM\n",
    " \n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Bidirectional\n",
    "from keras.layers.recurrent import LSTM\n",
    "import numpy as np\n",
    "# define model for simple BI-LSTM + DNN based binary classifier\n",
    "def define_model():\n",
    "    input1 = Input(shape=(2,2)) #use row and column size as input size\n",
    "    lstm1 = Bidirectional(LSTM(units=32))(input1)\n",
    "    dnn_hidden_layer1 = Dense(3, activation='relu')(lstm1)\n",
    "    dnn_output = Dense(1, activation='sigmoid')(dnn_hidden_layer1)\n",
    "    model = Model(inputs=[input1],outputs=[dnn_output])\n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "# Take a dummy 3D numpy array to train the model\n",
    "data = np.array([[[0.1, 0.15], [0.2, 0.25]],\n",
    "                 [[0.3, 0.35], [0.4, 0.45]],\n",
    "                 [[0.5, 0.55], [0.6, 0.65]],\n",
    "                 [[0.7, 0.75], [0.8, 0.85]],\n",
    "                 [[0.9, 0.95], [1.0, 1.5]]])\n",
    "Y = [1,1,1,0,0] #define binary class level for this model\n",
    "print(\"data = \", data)\n",
    "# NO NEED TO RESHAPE THE DATA as it is already in 3D format\n",
    "# Call the model\n",
    "model = define_model()\n",
    "# Fit the model\n",
    "model.fit([data],[np.array(Y)],epochs=4,batch_size=2,verbose=1)\n",
    "# Take a test data to test the working of the model\n",
    "test_data = np.array([[[0.2, 0.33],[0.2, 0.33]]])\n",
    "# predict the sigmoid output [0,1] for the 'test_data'\n",
    "pred = model.predict(test_data)\n",
    "print(\"predicted sigmoid output => \",pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
